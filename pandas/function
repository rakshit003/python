************ Reading data as date from external file ************************

Automatic
It should be noted that Pandas integrates powerful date parsers such that many different kinds of dates can be parsed automatically. 
Thus, you usually just need to set the parse_date parameter.

df = pd.read_csv("dates_text.csv", parse_dates=["date"])

if date values are spread across multiple columns then read like this

**data**
y,m,d,category,balance
2022,01,01,A,100
2022,02,02,B,200
2022,03,12,C,300

df_cols = pd.read_csv("dates_text_cols.csv", parse_dates={"date": ["y", "m", "d"]})

Also we can only read specific columns in data frame by using 'usecols' option like below:
df = pd.read_csv("/content/melb_housing.csv",usecols = ["Suburb", "Address", "Date","Distance", "Price"])

**customized date parser

pd.read_csv("custom_dt_fmt.csv", parse_dates=["date"], date_parser=lambda x: datetime.strptime(x, "%b_%d_%Y"))
The tricky thing here is to define the proper date format in the strptime method.

****missing values**********

sun all missing values group by respective colums
df.isna().sum()

****************processing dates in pandas*********

>>> pd.to_datetime("Jan 01, 2022") #to_datetime function converts any date string to pandas datetime object
Timestamp('2022-01-01 00:00:00')
>>> pd.to_datetime(["01/01/2022", "01/02/2022", "01/03/2022"])
DatetimeIndex(['2022-01-01', '2022-01-02', '2022-01-03'], dtype='datetime64[ns]', freq=None)
>>> pd.to_datetime(pd.Series(["01/01/2022", "01/02/2022", "01/03/2022"]))

pd.to_datetime('20190108',format='%Y%d%m')
With exact=False Pandas tries to match the pattern anywhere in the date string.
print(pd.to_datetime('yolo 20190108', format='%Y%d%m', exact=False))
**creating date range in pandas

pd.date_range(start="12/01/2022", end="12/07/2022")
DatetimeIndex(['2022-12-01', '2022-12-02', '2022-12-03', '2022-12-04',
               '2022-12-05', '2022-12-06', '2022-12-07'],

**sepeating date from time using dt class
>>> df["date"] = df["timestamp"].dt.date
>>> df["time"] = df["timestamp"].dt.time

or dt.month, dt.year, dt.weekday -- all are possible
provide custom format


**************iterating over dataframes**************
for index, row in df.iterrows():
  print(row["firstname"])
  
**********indexing*******************
iloc is conceptually simpler than loc because it ignores the dataset's indices. 
When we use iloc we treat the dataset like a big matrix (a list of lists), one that we have to index into by position. 
loc, by contrast, uses the information in the indices to do its work. 


***Filtering

reviews.loc[(reviews.country == 'Italy') & (reviews.points >= 90)]

Filtering multiple values
#if you have a lot of values to filter on a column the use isin

reviews.loc[reviews.country.isin(['Italy', 'France'])]
reviews.loc[reviews.price.notnull()]
reviews.loc[reviews.price.isnull()]

****************Assigning values************************

reviews['critic'] = 'everyone' #constant value
reviews['index_backwards'] = range(len(reviews), 0, -1) #range

*****apply and map functions**********
Maps allow us to transform data in a DataFrame or Series one value at a time for an entire column.

If we had called reviews.apply() with axis='index', 
then instead of passing a function to transform each row, 
we would need to give a function to transform each column.

Note that map() and apply() return new, transformed Series and DataFrames, respectively. 
They don't modify the original data they're called on. 
If we look at the first row of reviews, we can see that it still has its original points value.

def remean_points(row):
    row.points = row.points - review_points_mean
    return row

reviews.apply(remean_points, axis='columns')

review_points_mean = reviews.points.mean()
reviews.points.map(lambda p: p - review_points_mean)

*********group by operator*************
groupby() created a group of reviews which allotted the same point values to the given wines.
Then, for each of these groups, we grabbed the points() column and counted how many times it appeared. 
value_counts() is just a shortcut to this groupby() operation.

We can use any of the summary functions we've used before with this data. 
For example, to get the cheapest wine in each point value category, we can do the following:
reviews.groupby('points').price.min()

You can think of each group we generate as being a slice of our DataFrame containing only data with values that match. 

**apply mehtod on group by
This DataFrame is accessible to us directly using the apply() method, and we can then manipulate the data in any way we see fit. 

For example, here's one way of selecting the name of the first wine reviewed from each winery in the dataset:
reviews.groupby('winery').apply(lambda df: df.title.iloc[0]) #instead of a row entire data frame slice of the group is passed

df_rank=df.groupby('claim_num').apply(lambda df:  df.sort_values('change_date',ascending=False).iloc[0])


**group by with agg**
Another groupby() method worth mentioning is agg(), which lets you run a bunch of different functions on your DataFrame simultaneously. 
For example, we can generate a simple statistical summary of the dataset as follows:
reviews.groupby(['country']).price.agg([len, min, max])

group columns are created as multi index, inorder to reset them yo normal columns use below command
countries_reviewed.reset_index()


/**********merging two data frame **********/

pd.merge(df1,df2,on='col_name', how ='inner')

/********* Data Types and data conversion**************/

There are 7 data types in pandas

object : This data type is used for strings (i.e., sequences of characters)
int64 : Used for integers (whole numbers, no decimals)
float64 : Used for floating-point numbers (i.e., figures with decimals/fractions)
bool : Used for values that can only be True/False
datetime64 : Used for date and time values
timedelta : Used to represent the difference between datetimes
category : Used for values that take one out of a limited number of available options (categories donâ€™t have to, but can have explicit ordering)


Converting data types
There are two standard ways of converting pandas data types:

<column>.astype(<desired type>)
conversion helper functions, like pd.to_numeric or pd.to_datetime

invoices['Date'] = invoices['Date'].astype('datetime64')
invoices['Date'] = pd.todatetime(invoices['Date'])

Error in value type
if while converting we get an error which means few values in the field are of a datatype that can not be converted
we can use below command to find on how many values are these

invoices['Meal Price'].apply(lambda x: type(x)).value_counts()
invoices['Meal Price'][invoices['Meal Price'].apply(
  lambda x: isinstance(x,str).   #str for invalid strings 
)]

# convert the offending values into np.nan
invoices['Meal Price'] = pd.to_numeric(invoices['Meal Price'],errors='coerce')
# fill np.nan with the median of the data
invoices['Meal Price'] = invoices['Meal Price'].fillna(invoices['Meal Price'].median())
# convert the column into integer
invoices['Meal Price'].astype(int)


*****Accessar Methods of pandas*****
Pandas had accessars methods for a perticular data type you are trying to access

There are three different accessors:

dt
str
cat

invoices['Date of Meal'].dt.date
invoices['Date of Meal'].dt.weekday_name
invoices['Date of Meal'].dt.month_name()
invoices['Date of Meal'].dt.days_in_month

dt.
is_leap_year, is_month_start, is_month_end, is_quarter_start, is_quarter_end, is_year_start, is_year_end

We can use the results to filter our data down to only rows, where Date of Meal is at the month's end.

invoices[invoices['Date of Meal'].dt.is_month_end]

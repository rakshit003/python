***Good reads
https://www.kaggle.com/learn/pandas.  #basic pandas
https://towardsdatascience.com/learn-advanced-features-for-pythons-main-data-analysis-library-in-20-minutes-d0eedd90d086.  #advance pandas

************ Reading data as date from external file ************************

Automatic
It should be noted that Pandas integrates powerful date parsers such that many different kinds of dates can be parsed automatically. 
Thus, you usually just need to set the parse_date parameter.

df = pd.read_csv("dates_text.csv", parse_dates=["date"])

if date values are spread across multiple columns then read like this

**data**
y,m,d,category,balance
2022,01,01,A,100
2022,02,02,B,200
2022,03,12,C,300

df_cols = pd.read_csv("dates_text_cols.csv", parse_dates={"date": ["y", "m", "d"]})

Also we can only read specific columns in data frame by using 'usecols' option like below:
df = pd.read_csv("/content/melb_housing.csv",usecols = ["Suburb", "Address", "Date","Distance", "Price"])

**customized date parser

pd.read_csv("custom_dt_fmt.csv", parse_dates=["date"], date_parser=lambda x: datetime.strptime(x, "%b_%d_%Y"))
The tricky thing here is to define the proper date format in the strptime method.

****missing values**********

sun all missing values group by respective colums
df.isna().sum()

****************processing dates in pandas*********

>>> pd.to_datetime("Jan 01, 2022") #to_datetime function converts any date string to pandas datetime object
Timestamp('2022-01-01 00:00:00')
>>> pd.to_datetime(["01/01/2022", "01/02/2022", "01/03/2022"])
DatetimeIndex(['2022-01-01', '2022-01-02', '2022-01-03'], dtype='datetime64[ns]', freq=None)
>>> pd.to_datetime(pd.Series(["01/01/2022", "01/02/2022", "01/03/2022"]))

pd.to_datetime('20190108',format='%Y%d%m')
With exact=False Pandas tries to match the pattern anywhere in the date string.
print(pd.to_datetime('yolo 20190108', format='%Y%d%m', exact=False))
**creating date range in pandas

pd.date_range(start="12/01/2022", end="12/07/2022")
DatetimeIndex(['2022-12-01', '2022-12-02', '2022-12-03', '2022-12-04',
               '2022-12-05', '2022-12-06', '2022-12-07'],

**sepeating date from time using dt class
>>> df["date"] = df["timestamp"].dt.date
>>> df["time"] = df["timestamp"].dt.time

or dt.month, dt.year, dt.weekday -- all are possible
provide custom format

**************iterating over dataframes**************
for index, row in df.iterrows():
  print(row["firstname"])
  
**********indexing*******************
iloc is conceptually simpler than loc because it ignores the dataset's indices. 
When we use iloc we treat the dataset like a big matrix (a list of lists), one that we have to index into by position. 
loc, by contrast, uses the information in the indices to do its work. 


***Filtering

reviews.loc[(reviews.country == 'Italy') & (reviews.points >= 90)]

Filtering multiple values
#if you have a lot of values to filter on a column the use isin

reviews.loc[reviews.country.isin(['Italy', 'France'])]
reviews.loc[reviews.price.notnull()]
reviews.loc[reviews.price.isnull()]

****************Assigning values************************

reviews['critic'] = 'everyone' #constant value
reviews['index_backwards'] = range(len(reviews), 0, -1) #range

*****apply and map functions**********
Maps allow us to transform data in a DataFrame or Series one value at a time for an entire column.

If we had called reviews.apply() with axis='index', 
then instead of passing a function to transform each row, 
we would need to give a function to transform each column.

Note that map() and apply() return new, transformed Series and DataFrames, respectively. 
They don't modify the original data they're called on. 
If we look at the first row of reviews, we can see that it still has its original points value.

def remean_points(row):
    row.points = row.points - review_points_mean
    return row

reviews.apply(remean_points, axis='columns')

review_points_mean = reviews.points.mean()
reviews.points.map(lambda p: p - review_points_mean)

*********group by operator*************
groupby() created a group of reviews which allotted the same point values to the given wines.
Then, for each of these groups, we grabbed the points() column and counted how many times it appeared. 
value_counts() is just a shortcut to this groupby() operation. It converts series values into index and counts as values, 
                                                               so any index operators can be used on top of it

We can use any of the summary functions we've used before with this data. 
For example, to get the cheapest wine in each point value category, we can do the following:
reviews.groupby('points').price.min()

You can think of each group we generate as being a slice of our DataFrame containing only data with values that match. 

**apply mehtod on group by
This DataFrame is accessible to us directly using the apply() method, and we can then manipulate the data in any way we see fit. 

For example, here's one way of selecting the name of the first wine reviewed from each winery in the dataset:
reviews.groupby('winery').apply(lambda df: df.title.iloc[0]) #instead of a row entire data frame slice of the group is passed

df_rank=df.groupby('claim_num').apply(lambda df:  df.sort_values('change_date',ascending=False).iloc[0])


**group by with agg**
Another groupby() method worth mentioning is agg(), which lets you run a bunch of different functions on your DataFrame simultaneously. 
For example, we can generate a simple statistical summary of the dataset as follows:
reviews.groupby(['country']).price.agg([len, min, max])

group columns are created as multi index, inorder to reset them yo normal columns use below command
countries_reviewed.reset_index()


/**********merging two data frame **********/

***concat
pd.concat takes a couple of optional parameters next to the list of DataFrames that you call concat on:
axis : 0 for vertical, 1 for horizontal. axis defaults to 0
join : 'inner' for the intersection , join defaults to outer
When we use axis=0 and join='inner' we will consider only overlapping columns
When using axis=1 and join='inner' we consider only overlapping indices
In the case of outer non-overlapping columns/indices will be filled with nan values 

ignore_index : True to ignore preexisting indices and instead use labels from 0 to n-1 for the resulting DataFrame. ignore_index defaults to False
pd.concat(<LIST OF DATAFRAMES>). ##vertical concat

**merge

Merging, as opposed to concatenating DataFrames together, allows us to combine two DataFrames in a more traditional SQL-query kind of way
When merging DataFrames, most of the time you want some information from one source and another piece of information from another source. 
Whereas when concatenating your DataFrames are structurally and in terms of content quite similar, and you want to combine them into one unified DataFrame.

pd.merge(df1,df2,on='col_name', how ='inner')

pd.merge(
 left, 
 right, 
 how='inner', 
 on=None, 
 left_on=None, 
 right_on=None,         
 left_index=False,
 right_index=False, 
 sort=True,
 suffixes=('_x', '_y'), 
 copy=True, 
 indicator=False,
 validate=None
) -> pd.DataFrame

# left_index/right_index : If True, use the index from the left/right DataFrame to merge on. left_index/right_index defaults to False

#left_on/right_on : Column name(s) from the left/right DataFrame to join on. Typical use case: Keys you are joining on are differently labeled in your DataFrames. E.g., what is location_id in your left DataFrame, might be _id in your right DataFrame. 
In this case, you would do left_on='location_id', right_on='_id' 

#suffixes: A tuple of string suffixes to apply to overlapping columns. suffixes defaults to ('_x', '_y'). I like to use ('_base', '_joined')


if no parametres specified merge happens on all common columns

##on
If we do explicitly provide an on parameter this will override the default behavior and try to find the provided column in both DataFrames. 
Remaining duplicated columns that are not being used to merge on will be suffixed.

pd.merge(order_data,invoices,on='Order Id')

We can also specify custom suffixes like this: default suffix is name of original df
pd.merge(order_data,invoices,on='Order Id',suffixes=('_base','_join'))

#left_on , right_on
You would typically use the left_on and right_on parameters when the columns are named differently in the two DataFrames.

****join function
You might have seen the usage of jointo the same end asmerge. join by default merges on the index of both DataFrames. 
I advise against the usage of joinas it is just a special case of mergeand does not provide a benefit over merging.

/********* Data Types and data conversion**************/

There are 7 data types in pandas

object : This data type is used for strings (i.e., sequences of characters)
int64 : Used for integers (whole numbers, no decimals)
float64 : Used for floating-point numbers (i.e., figures with decimals/fractions)
bool : Used for values that can only be True/False
datetime64 : Used for date and time values
timedelta : Used to represent the difference between datetimes
category : Used for values that take one out of a limited number of available options (categories donâ€™t have to, but can have explicit ordering)


Converting data types
There are two standard ways of converting pandas data types:

<column>.astype(<desired type>)
conversion helper functions, like pd.to_numeric or pd.to_datetime

invoices['Date'] = invoices['Date'].astype('datetime64')
invoices['Date'] = pd.todatetime(invoices['Date'])

Error in value type
if while converting we get an error which means few values in the field are of a datatype that can not be converted
we can use below command to find on how many values are these

invoices['Meal Price'].apply(lambda x: type(x)).value_counts()
invoices['Meal Price'][invoices['Meal Price'].apply(
  lambda x: isinstance(x,str).   #str for invalid strings 
)]

# convert the offending values into np.nan
invoices['Meal Price'] = pd.to_numeric(invoices['Meal Price'],errors='coerce')
# fill np.nan with the median of the data
invoices['Meal Price'] = invoices['Meal Price'].fillna(invoices['Meal Price'].median())
# convert the column into integer
invoices['Meal Price'].astype(int)


*****Accessar Methods of pandas*****
Pandas had accessars methods for a perticular data type you are trying to access

There are three different accessors:

dt
str
cat


**dt accessor

invoices['Date of Meal'].dt.date
invoices['Date of Meal'].dt.weekday_name
invoices['Date of Meal'].dt.month_name()
invoices['Date of Meal'].dt.days_in_month

dt.
is_leap_year, is_month_start, is_month_end, is_quarter_start, is_quarter_end, is_year_start, is_year_end

We can use the results to filter our data down to only rows, where Date of Meal is at the month's end.

invoices[invoices['Date of Meal'].dt.is_month_end]

invoices['Date of Meal'].dt.year.value_counts().sort_index(). #another handy example

to_pydatetime(), which converts the Pandas datetime into a regular Python datetime format (which you might need sometimes) and
invoices['Date of Meal'].dt.to_pydatetime()


** str accessor

invoices['Type of Meal'].str.lower()
invoices['Type of Meal'].str.split(',')  # splits a string seperated by commas into list of strings, 
invoices['Type of Meal'].str.split(',').apply(lambda x: x[0]) # you can use apply function to access the list
ljust(width), rjust(width), center(width), zfill(width) to control the positioning of strings. 
All of them take a total width of the desired resulting string as an input. ljust, rjust, 
and center fill the difference to the desired length with whitespaces.zfill adds that many leading zeroes. 
ljust is left-bound, rjustis right-bound.

startswith(<substring>), endswith(<substring>), contains(<substring>) checks for the presence of a substring
this bulean values can then be used to subset a df

**cast accessor

invoices['Type of Meal'].cat.categories
invoices['Type of Meal'].cat.codes #codes for quick conversion of the category into its numerical representation (underlying order of ordered list)
invoices['Type of Meal'].cat.reorder_categories(['Lunch','Breakfast','Dinner'])

*********************

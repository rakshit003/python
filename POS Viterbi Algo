##Viterbi Algorith is a graph algorithm , it utilize the transition and emmision metrices of Markov Model
### this is used to find the POS tags of each word in a sentence 
#### transition prob can be mulitplied with emission probability to find the final probability of occurance of a word
##### for occurance of an entire sentance, a gprah is built to find the best possible paths through these transitions and emission

Vterbi Algo has 3 steps:

Initialization - In this part you initialize the best_paths and best_probabilities matrices that you will be populating in feed_forward.(only first column of each metrices)
Feed forward - At each step, you calculate the probability of each path happening and the best paths up to that point.
Feed backward: This allows you to find the best path with the highest probabilities.

Matrics C and D are calulcated (POSn * Nsentence word)
C :This matrix will have the probabilities that will tell you what part of speech each word belongs to.  
D Tracks keep track what part of speech you are coming from. ( save the index of tag which maimizes the final probability saved in C matrics, keep in mind the preious 
  state prob and the transition probability from prev satet to current state)
ci,j=maxkck,j−1∗ak,i∗bi,cindex(wj)

## index is nothing but the state number(POS tag number)


####Initialization step

# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)
# GRADED FUNCTION: initialize
def initialize(states, tag_counts, A, B, corpus, vocab):
    '''
    Input: 
        states: a list of all possible parts-of-speech
        tag_counts: a dictionary mapping each tag to its respective count
        A: Transition Matrix of dimension (num_tags, num_tags)
        B: Emission Matrix of dimension (num_tags, len(vocab))
        corpus: a sequence of words whose POS is to be identified in a list 
        vocab: a dictionary where keys are words in vocabulary and value is an index
    Output:
        best_probs: matrix of dimension (num_tags, len(corpus)) of floats
        best_paths: matrix of dimension (num_tags, len(corpus)) of integers
    '''
    # Get the total number of unique POS tags
    num_tags = len(tag_counts)
    
    # Initialize best_probs matrix 
    # POS tags in the rows, number of words in the corpus as the columns
    best_probs = np.zeros((num_tags, len(corpus)))
    
    # Initialize best_paths matrix
    # POS tags in the rows, number of words in the corpus as columns
    best_paths = np.zeros((num_tags, len(corpus)), dtype=int)
    
    # Define the start token
    s_idx = states.index("--s--")
    ### START CODE HERE (Replace instances of 'None' with your code) ###
    
    # Go through each of the POS tags
    for i in range(num_tags): # complete this line
        
        # Handle the special case when the transition from start token to POS tag i is zero
        if A[s_idx,i]==0: # complete this line
            
            # Initialize best_probs at POS tag 'i', column 0, to negative infinity
            best_probs[i,0] = float('-inf')
        
        # For all other cases when transition from start token to POS tag i is non-zero:
        else:
            
            # Initialize best_probs at POS tag 'i', column 0
            # Check the formula in the instructions above
            best_probs[i,0] = math.log(A[s_idx,i])+math.log(B[i,0])
                        
    ### END CODE HERE ### 
    return best_probs, best_paths
    
    best_probs, best_paths = initialize(states, tag_counts, A, B, prep, vocab)
    
    # Test the function
print(f"best_probs[0,0]: {best_probs[0,0]:.4f}") 
print(f"best_paths[2,3]: {best_paths[2,3]:.4f}")

############Forward pass

### i-- represnts ith word in a sequenced corpus
### k-- Represnts the kth tag of the previous state
### j-- Represents the jth tag of the current state





